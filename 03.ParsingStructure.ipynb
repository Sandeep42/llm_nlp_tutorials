{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a6fbcc-fb93-4fd7-9f64-e1fe501b0563",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8195cdfa-9642-4ecf-a85f-9daa6badae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import List, Literal, Annotated\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "import getpass\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from prompt_poet import Prompt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from devtools import pprint\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfd808f-5c94-4ee6-9c34-28605a0c4ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "groq_api_key = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4d071b-2885-4201-bb0e-f83fd51c87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "def download_arxiv_pdf(arxiv_id, save_path):\n",
    "    # Construct the arXiv PDF URL\n",
    "    url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    \n",
    "    # Send a GET request to download the PDF\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content of the response (PDF) to a file\n",
    "        with open(save_path, 'wb') as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        print(f\"PDF downloaded successfully and saved to {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "\n",
    "        # Loop through all the pages\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c057e7a-6f67-4638-8ff5-e4c76b0705b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded successfully and saved to tmp/1706.03762.pdf\n"
     ]
    }
   ],
   "source": [
    "download_arxiv_pdf(\"1706.03762\",\"tmp/1706.03762.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e93e7c-0267-4ec5-8e6f-297374837b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_from_pdf(\"tmp/1706.03762.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8988a5-a281-40ee-8c9e-f9c9768715b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaise\n"
     ]
    }
   ],
   "source": [
    "print(pdf_text[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6269a01-4367-4605-a0b1-c81aeb4e8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# estimating how long the document is\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def estimate_context_length(text):\n",
    "    # Load GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # Return the number of tokens\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6bc9ef5-1cb0-481b-884b-458451cdecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandeep/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10540 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10540"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_context_length(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321d367-240a-433a-9aa6-78a4c9aae5b9",
   "metadata": {},
   "source": [
    "Can we parse the structure of the paper, extract title, abstract, sections and their subsections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369d794-1ff1-456b-8be9-b1d83303e763",
   "metadata": {},
   "source": [
    "#### Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3df9938-802a-452a-b12d-1b67ebb51765",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_template = \"\"\"\n",
    "- name: system instructions\n",
    "  role: system\n",
    "  content: |\n",
    "   You are an expert in parsing a given research article into title, abstract, section and subsections.\n",
    "\n",
    "- name: user query\n",
    "  role: user\n",
    "  content: |\n",
    "   Please extract properties defined in 'Parser' function from the text.\n",
    "   {{ escape_special_characters(text) }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4c147be8-230d-462c-b930-89863134ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subsection(BaseModel):\n",
    "    subsection_title: Optional[str] = Field(default=None,description=\"Title of subsection. Keep it short and within 2-3 words\")\n",
    "\n",
    "class Section(BaseModel):\n",
    "    section_title: Optional[str] = Field(default=None,description=\"Title of section. Keep it short and within 2-3 words.\")\n",
    "    # subsections: Optional[List[Subsection]] = Field(default=None,description=\"List of titles of sub-sections within the section if any\")\n",
    "    \n",
    "class Parser(BaseModel):\n",
    "    title: str = Field(default=None,description=\"The title of the research article\")\n",
    "    authors: List[str] = Field(default=None,description=\"List of Authors of the research article, keep only the name\")\n",
    "    abstract: str = Field(default=None,description=\"Abstract of the research article\")\n",
    "    sections: Optional[List[Section]] = Field(default=None,description=\"Title of the sections mentioned in the text. Keep it short and within 2-3 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df0a3a82-fa33-4e36-bd8a-f4c1b1cfcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = Prompt(\n",
    "    raw_template=raw_template,\n",
    "    template_data={\"text\": pdf_text[0:1000]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2e512532-8f74-425d-ae09-fc3615f60485",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_parser = ChatGroq(temperature=0, model_name=\"llama3-groq-70b-8192-tool-use-preview\",\n",
    "                      api_key=groq_api_key).with_structured_output(Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "846ed14e-22e4-4b9e-b73f-bfcd23a05db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_parser.invoke(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64bd69e7-6b80-4413-b177-ce3c6b11752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser(\n",
      "    title='Attention Is All You Need',\n",
      "    authors=[\n",
      "        'Ashish Vaswani',\n",
      "        'Noam Shazeer',\n",
      "        'Niki Parmar',\n",
      "        'Jakob Uszkoreit',\n",
      "        'Llion Jones',\n",
      "        'Aidan N. Gomez',\n",
      "        'Łukasz Kaiser',\n",
      "        'Illia Polosukhin',\n",
      "    ],\n",
      "    abstract=(\n",
      "        'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks tha'\n",
      "        't include an encoder and a decoder. The best performing models also connect the encoder and decoder through a'\n",
      "        'n attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attenti'\n",
      "        'on mechanisms, dispensing with recurrence and convolutions entirely. Experime'\n",
      "    ),\n",
      "    sections=[\n",
      "        Section(\n",
      "            section_title='Abstract',\n",
      "        ),\n",
      "    ],\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e523654-449d-4655-a6e3-b4a2bdd73e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = Prompt(\n",
    "    raw_template=raw_template,\n",
    "    template_data={\"text\": pdf_text}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ebfcf81-0db2-4e03-ba87-3877aa43126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = llm_parser.invoke(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eae81a-fbb5-442c-a35c-3a8f9349c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec661c39-d7f7-490e-baa7-429a1ea994b3",
   "metadata": {},
   "source": [
    "There are two problems here.\n",
    "\n",
    "1. Context Length too large\n",
    "2. Too many tasks in a single prompt\n",
    "3. We don't see the structure of the text after parsing (Parsing PDFs is not easy)\n",
    "\n",
    "What if we read page by page.\n",
    "\n",
    "Or even better option is to use divide and conquer.\n",
    "\n",
    "We can one prompt for extraction, another prompt for sections, another one for sub-sections etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480783e-4644-4050-b5de-f0ce967b147d",
   "metadata": {},
   "source": [
    "#### Mutli-Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "294b8ec0-d7e1-454d-a02b-e492b4863123",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_0_template = \"\"\"\n",
    "- name: system instructions\n",
    "  role: system\n",
    "  content: |\n",
    "   You are an expert in parsing a given research article into title, abstract, section and subsections.\n",
    "   Since I can not pass the whole document in a single iteration, I will pass text page by page.\n",
    "   This is the first page. Read the page and understand what components do you need to do this task. \n",
    "   Keep in memory that the output from first page will be passed on to you when you process next page.\n",
    "\n",
    "- name: user query\n",
    "  role: user\n",
    "  content: |\n",
    "   Please extract title, abstract, sections, subsections from the following text.\n",
    "   {{ escape_special_characters(text) }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9d3e65ce-5cd7-4d5a-a83c-0376f6ba4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_page_by_page(pdf_path):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "\n",
    "        # Loop through all the pages\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            yield page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2d1cc45a-dab3-4a95-83a2-77e0edd4e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_gen = read_page_by_page(\"tmp/1706.03762.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a3fe3f62-37da-4e1a-b968-c88856b9ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_base = ChatGroq(temperature=0, model_name=\"llama3-groq-70b-8192-tool-use-preview\",\n",
    "                      api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2de48246-5b53-4395-9e13-59fff9d8f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_0 = next(page_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d66d6289-a990-40fa-8f68-5b3c9907a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_0 = Prompt(\n",
    "    raw_template=prompt_0_template,\n",
    "    template_data={\"text\": page_0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c8d6e3a4-3e47-4322-829f-dcb6732c3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "page0_resp = llm_base.invoke(prompt_0.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af5bf9cb-ab27-4035-adee-5b1b450a975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_x_template = \"\"\"\n",
    "- name: system instructions\n",
    "  role: system\n",
    "  content: |\n",
    "   You are an expert in parsing a given research article into title, abstract, section and subsections.\n",
    "   Since I can not pass the whole document in a single iteration, I will pass text page by page.\n",
    "   This is the not the first page. Read the page and understand what components do you need to do this task. \n",
    "   The output from previous page is given below.\n",
    "   {{ escape_special_characters(prev_page_response) }}\n",
    "\n",
    "- name: user query\n",
    "  role: user\n",
    "  content: |\n",
    "   Please extract title, abstract, sections, subsections from the following text.\n",
    "   {{ escape_special_characters(text) }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "380fbe2f-0240-4ab0-b7e4-cdab0ac5a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_1 = next(page_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "04cb2af4-23a1-44e9-9ee1-f96b8cd686e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = Prompt(\n",
    "    raw_template=prompt_x_template,\n",
    "    template_data={\"text\": page_1, \"prev_page_response\": page0_resp.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2964e4d4-42ed-486f-baf6-4f8afccaf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1_resp = llm_base.invoke(prompt_1.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "62f67bc8-234d-456e-9730-23c089424ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Attention Is All You Need\n",
      "\n",
      "Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "Section 1: Introduction\n",
      "- Subsection 1.1: Recurrent Neural Networks\n",
      "- Subsection 1.2: Self-Attention\n",
      "- Subsection 1.3: Model Architecture\n",
      "\n",
      "Section 2: Background\n",
      "- Subsection 2.1: Convolutional Neural Networks\n",
      "- Subsection 2.2: Self-Attention\n",
      "- Subsection 2.3: Model Architecture\n",
      "\n",
      "Section 3: Model Architecture\n",
      "- Subsection 3.1: Encoder-Decoder Structure\n",
      "- Subsection 3.2: Auto-Regressive Model\n",
      "- Subsection 3.3: Attention Mechanism\n"
     ]
    }
   ],
   "source": [
    "print(page1_resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f38651-bb1c-4fbf-a955-eaee40a78f8e",
   "metadata": {},
   "source": [
    "Looks like it is working. Let me simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "950c9884-56f3-45a1-9f1e-bd5975058836",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_prompt = \"\"\"\n",
    "- name: system instructions\n",
    "  role: system\n",
    "  content: |\n",
    "   You are an expert in parsing a given research article into title, abstract, section and subsections.\n",
    "   Since I can not pass the whole document in a single iteration, I will pass text page by page.\n",
    "   {% if page_number == 1 %}\n",
    "   This is the first page. Read the page and understand what components do you need to do this task. \n",
    "   Keep in memory that the output from first page will be passed on to you when you process next page.\n",
    "   Only keep the text that you need. Keep it short.\n",
    "   {% endif %}\n",
    "   {% if page_number != 1 %}\n",
    "   This is the not the first page. Read the current page and understand what components do you need to do this task. \n",
    "   The output from previous page is given below.\n",
    "    Only keep the text that you need. Keep it short.\n",
    "   {{ escape_special_characters(prev_page_response) }}\n",
    "   {% endif %}\n",
    "\n",
    "- name: user query\n",
    "  role: user\n",
    "  content: |\n",
    "   Please extract title, abstract, sections, subsections from the following text.\n",
    "   {{ escape_special_characters(current_page) }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a08349f-b16f-4859-89c9-0eeaf81b28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_gen = read_page_by_page(\"tmp/1706.03762.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "441b3b0e-c5de-4bc1-9968-bf897fc0a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(page_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7dbbbff5-0564-44aa-84e6-581741f98746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ccc96198-71cc-4e43-b4e3-81c546c62769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 15/15 [02:19<00:00,  9.27s/it]\n"
     ]
    }
   ],
   "source": [
    "current_resp = ''\n",
    "i =1\n",
    "for page in tqdm(pages):\n",
    "    prompt = Prompt(\n",
    "    raw_template=parser_prompt,\n",
    "    template_data={\"prev_page_response\": current_resp, \"current_page\": page, \"page_number\" : i}\n",
    "    )\n",
    "    resp = llm_base.invoke(prompt.messages)\n",
    "    # print(resp.content)\n",
    "    current_resp += resp.content\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ff80bf4b-0edc-4fc6-afb9-58bfb9c033c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Attention Is All You Need\n",
      "Abstract: We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.Title: Attention Is All You Need\n",
      "Abstract: We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "\n",
      "Section 1: Introduction\n",
      "- Discusses the use of recurrent neural networks and their limitations in sequence modeling and transduction problems.\n",
      "- Introduces the Transformer model as a new approach that relies entirely on self-attention mechanisms.\n",
      "\n",
      "Section 2: Background\n",
      "- Describes other models that aim to reduce sequential computation, such as the Extended Neural GPU, ByteNet, and ConvS2S.\n",
      "- Discusses the limitations of these models and how the Transformer addresses these issues with Multi-Head Attention.\n",
      "\n",
      "Section 3: Model Architecture\n",
      "- Describes the general structure of neural sequence transduction models, including an encoder-decoder structure.\n",
      "- Introduces the Transformer's encoder and decoder components, which rely on self-attention mechanisms for generating output sequences.Title: The Transformer Model Architecture\n",
      "Abstract: The Transformer model follows an overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. It consists of a stack of N=6 identical layers, each with two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The decoder also includes a third sub-layer that performs multi-head attention over the output of the encoder stack.\n",
      "\n",
      "Section 3: Model Architecture\n",
      "- 3.1 Encoder and Decoder Stacks: Describes the structure of the encoder and decoder, including the use of residual connections and layer normalization.\n",
      "- 3.2 Attention: Defines the attention function and its application in the Transformer model.Title: Scaled Dot-Product Attention and Multi-Head Attention\n",
      "Abstract: This section describes the Scaled Dot-Product Attention and Multi-Head Attention mechanisms used in the Transformer model. It explains how these mechanisms are applied to queries, keys, and values to compute attention weights and outputs.\n",
      "\n",
      "Section 3.2: Attention\n",
      "- 3.2.1 Scaled Dot-Product Attention: Describes the Scaled Dot-Product Attention function, which computes the dot products of the query with all keys, divides each by √dk, and applies a softmax function to obtain the weights on the values.\n",
      "- 3.2.2 Multi-Head Attention: Introduces the Multi-Head Attention mechanism, which projects queries, keys, and values multiple times with different linear projections and performs the attention function in parallel.Title: Multi-Head Attention and Position-wise Feed-Forward Networks\n",
      "Abstract: This section describes the multi-head attention mechanism and its application in the Transformer model. It also explains the position-wise feed-forward networks used in the encoder and decoder layers.\n",
      "\n",
      "Section 3.2: Multi-Head Attention\n",
      "- 3.2.1 Scaled Dot-Product Attention: Describes the Scaled Dot-Product Attention function and its application in the Transformer model.\n",
      "- 3.2.2 Multi-Head Attention: Introduces the Multi-Head Attention mechanism, which allows the model to attend to different representation subspaces at different positions.\n",
      "- 3.2.3 Applications of Attention in our Model: Describes the use of multi-head attention in three different ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder.\n",
      "\n",
      "Section 3.3: Position-wise Feed-Forward Networks\n",
      "- Describes the fully connected feed-forward networks used in the encoder and decoder layers, which apply two linear transformations with a ReLU activation in between.\n",
      "\n",
      "Section 3.4: Embeddings and Softmax\n",
      "- Describes the use of learned embeddings to convert input and output tokens to vectors of dimension dmodel.\n",
      "- Explains the use of a shared weight matrix between the two embedding layers and the pre-softmax linear transformation.Title: Why Self-Attention\n",
      "Abstract: This section compares various aspects of self-attention layers to recurrent and convolutional layers, highlighting the benefits of self-attention in terms of computational complexity, parallelization, and learning long-range dependencies.\n",
      "\n",
      "Section 4: Why Self-Attention\n",
      "- 4.1 Computational Complexity: Compares the computational complexity of self-attention layers to recurrent and convolutional layers, showing that self-attention layers are faster for long sequences.\n",
      "- 4.2 Parallelization: Discusses the parallelization capabilities of self-attention layers, which can be computed in parallel, unlike sequential operations in recurrent layers.\n",
      "- 4.3 Learning Long-Range Dependencies: Explains how self-attention layers can learn long-range dependencies more easily due to shorter path lengths between input and output positions.Title: Training the Transformer Model\n",
      "Abstract: This section describes the training regime for the Transformer models, including the use of byte-pair encoding, batching, hardware, and schedule. It also discusses the optimizer, learning rate, and regularization techniques used during training.\n",
      "\n",
      "Section 5: Training\n",
      "- 5.1 Training Data and Batching: Describes the use of the WMT 2014 English-German and English-French datasets, byte-pair encoding, and batching techniques.\n",
      "- 5.2 Hardware and Schedule: Discusses the hardware used for training and the training schedule.\n",
      "- 5.3 Optimizer: Describes the use of the Adam optimizer and the learning rate formula.\n",
      "- 5.4 Regularization: Discusses the three types of regularization techniques used during training.Title: Model Variations and Results\n",
      "Abstract: This section discusses the importance of different components of the Transformer model by varying the base model in different ways and measuring the change in performance on English-to-German translation. It also summarizes the results and compares the translation quality and training costs to other model architectures from the literature.\n",
      "\n",
      "Section 6: Results\n",
      "- 6.1 Machine Translation: Describes the performance of the Transformer model on the WMT 2014 English-to-German and English-to-French translation tasks, comparing it to previous state-of-the-art models and ensembles.\n",
      "- 6.2 Model Variations: Evaluates the importance of different components of the Transformer model by varying the base model in different ways and measuring the change in performance on English-to-German translation.Title: Model Variations and Results\n",
      "Abstract: This section discusses the importance of different components of the Transformer model by varying the base model in different ways and measuring the change in performance on English-to-German translation. It also summarizes the results and compares the translation quality and training costs to other model architectures from the literature.\n",
      "\n",
      "Section 6.3: English Constituency Parsing\n",
      "- 6.3.1 Introduction: Evaluates if the Transformer can generalize to other tasks by performing experiments on English constituency parsing.\n",
      "- 6.3.2 Experiments: Describes the training of a 4-layer transformer on the Wall Street Journal (WSJ) portion of the Penn Treebank and in a semi-supervised setting using the larger high-confidence and BerkleyParser corpora.Title: Model Variations and Results\n",
      "Abstract: This section discusses the importance of different components of the Transformer model by varying the base model in different ways and measuring the change in performance on English-to-German translation. It also summarizes the results and compares the translation quality and training costs to other model architectures from the literature.\n",
      "\n",
      "Section 6: Conclusion\n",
      "- 6.1 Summary: Summarizes the key contributions of the Transformer model, including its ability to be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "- 6.2 Future Work: Discusses future research goals, including applying the Transformer to other tasks and extending it to handle large inputs and outputs such as images, audio, and video.Title: References\n",
      "Abstract: This section provides a list of references cited in the paper, including various studies on neural networks, sequence modeling, and machine translation.\n",
      "\n",
      "Section 1: References\n",
      "- [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n",
      "- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\n",
      "- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\n",
      "- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n",
      "- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n",
      "- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n",
      "- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\n",
      "- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\n",
      "- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\n",
      "- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\n",
      "- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\n",
      "- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n",
      "- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n",
      "- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "- [24] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.Title: References\n",
      "Abstract: This section provides a list of references cited in the paper, including various studies on neural networks, sequence modeling, and machine translation.\n",
      "\n",
      "Section 1: References\n",
      "- [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "- [26] David McClosky, Eugene Charniak, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "- [28] Romain Paulus, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational\n",
      "Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July 2006.\n",
      "- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\n",
      "arXiv:1608.05859 , 2016.\n",
      "- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909 , 2015.\n",
      "- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv\n",
      "preprint arXiv:1701.06538 , 2017.\n",
      "- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research , 15(1):1929–1958, 2014.\n",
      "- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates, Inc., 2015.\n",
      "- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\n",
      "- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n",
      "- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems , 2015.\n",
      "- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144 , 2016.\n",
      "- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, Wei Xu, and Jingbo Zhu. Deep recurrent models\n",
      "with fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long\n",
      "Papers) , pages 434–443. ACL, August 2013.Title: Attention Visualizations\n",
      "Abstract: This section provides an example of the attention mechanism in the Transformer model, specifically in the encoder self-attention layer 5 of 6. It highlights how many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. The figure illustrates the attention weights for the word 'making' and demonstrates the use of different colors to represent different heads.\n",
      "\n",
      "Section 1: Introduction\n",
      "- 1.1 Background: Introduces the concept of attention visualizations and their importance in understanding the Transformer model's behavior.\n",
      "\n",
      "Section 2: Attention Visualizations\n",
      "- 2.1 Example Visualization: Provides an example of the attention mechanism in the Transformer model, specifically in the encoder self-attention layer 5 of 6.\n",
      "- 2.2 Analysis: Analyzes the attention weights for the word 'making' and demonstrates how different heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'.Title: The Imperfections of the Law and Its Application\n",
      "\n",
      "Abstract: This section discusses the imperfections of the law and its application, highlighting the importance of understanding that the law will never be perfect but its application should be just. It emphasizes the need for a more just application of the law, which is currently missing.\n",
      "\n",
      "Section 1: Introduction\n",
      "- 1.1 Background: Introduces the concept of the imperfections of the law and its application, and the importance of a more just application.\n",
      "\n",
      "Section 2: Imperfections of the Law\n",
      "- 2.1 The Imperfections of the Law: Discusses the inherent imperfections of the law and how it can never be perfect.\n",
      "- 2.2 Importance of Just Application: Highlights the importance of a more just application of the law, which is currently missing.\n",
      "\n",
      "Section 3: Conclusion\n",
      "- 3.1 Summary: Summarizes the key points about the imperfections of the law and the need for a more just application.\n",
      "- 3.2 Future Work: Discusses potential future research directions for improving the application of the law.Title: The Imperfections of the Law and Its Application\n",
      "\n",
      "Abstract: The law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\n",
      "\n",
      "Section 1: Introduction\n",
      "- 1.1 Background: Introduces the concept of the imperfections of the law and its application, and the importance of a more just application.\n",
      "\n",
      "Section 2: Imperfections of the Law\n",
      "- 2.1 The Imperfections of the Law: Discusses the inherent imperfections of the law and how it can never be perfect.\n",
      "- 2.2 Importance of Just Application: Highlights the importance of a more just application of the law, which is currently missing.\n",
      "\n",
      "Section 3: Conclusion\n",
      "- 3.1 Summary: Summarizes the key points about the imperfections of the law and the need for a more just application.\n",
      "- 3.2 Future Work: Discusses potential future research directions for improving the application of the law.\n"
     ]
    }
   ],
   "source": [
    "print(current_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b7376520-7ff7-4f8f-969c-a1a12ece5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = Prompt(\n",
    "    raw_template=raw_template,\n",
    "    template_data={\"text\": current_resp}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "84a5957d-519c-4d2f-ab59-252f0b8a6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_parser.invoke(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e5886470-f179-4a35-bb33-17f073052a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser(\n",
      "    title='Attention Is All You Need',\n",
      "    authors=[\n",
      "        'Ashish Vaswani',\n",
      "        'Noam Shazeer',\n",
      "        'Niki Parmar',\n",
      "        'Llion Jones',\n",
      "        'Aidan Gomez',\n",
      "        'Lukasz Kaiser',\n",
      "        'Illia Polosukhin',\n",
      "    ],\n",
      "    abstract=(\n",
      "        'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensi'\n",
      "        'ng with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models '\n",
      "        'to be superior in quality while being more parallelizable and requiring significantly less time to train.'\n",
      "    ),\n",
      "    sections=[\n",
      "        Section(\n",
      "            section_title='Introduction',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='Background',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='Model Architecture',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='Why Self-Attention',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='Training the Transformer Model',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='Model Variations and Results',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='Attention Visualizations',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='The Imperfections of the Law and Its Application',\n",
      "        ),\n",
      "        Section(\n",
      "            section_title='References',\n",
      "        ),\n",
      "    ],\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161f0e4-0cfb-4faf-a60b-6cf8b4533d83",
   "metadata": {},
   "source": [
    "Looks like the mentions of Law triggered the LLM to bring out noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a79b6-b786-44b5-99ba-3d772e7c9822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
